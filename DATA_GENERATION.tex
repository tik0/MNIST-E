\section{Introduction}
\label{sec:data_generation}
%
It is quite common in the multi-modal VAE community to model a bi-modal dataset as follows (\cite{Wang2016_2,Ngiam2011,Suzuki2017,Vedantam2017}):
%
The first modality $\a$ denotes the raw data and $\b$ denotes the label (e.g. the digits' images and labels as one-hot vector wrt. the MNIST dataset).
%
This is a rather artificial assumption and only sufficient when the objective is within a semi-supervised training framework.
%
Real multi-modal data does not show this behavior as there are commonly multiple raw data inputs.
%
Unfortunately, only complex multi-modal datasets of heterogeneous sensor setups exist (\cite{Ofli2013,udacity2016,kragh2017fieldsafe}), which makes a comprehensive evaluation for VAEs futile.
%
On the other hand, creating own multi-modal datasets is exhaustive since  training generative models either demand dense sampling or supervised signals to form a consistent latent manifold (\cite{bengio2012}).
%one either needs a dense data-set or have to create labeled data, because
%
%This ambiguity may be resolved through either dense sampling of the manifolds or by adding supervised signals. 
%
%The importance of vast quantities of unlabeled data for the success of unsupervised approaches in learning disentangled factor representations was pointed by \cite{bengio2012}.

While na\"ive consolidation of non-coherently datasets does not meet the conditions of data continuity, as discussed later, we propose a consolidation technique by sampling from superimposed latent spaces of various uni-modal trained CVAEs in Sec. \ref{sec:mm_set_generation}.
%
This approach allows the generation of multi-modal datasets from distinct and disconnected uni-modal sets.
%

\subsection{MNIST-E}
\label{sec:mm_set_generation}

\cite{Perry2010} state that Hebbian learning relies on the fact that the same objects are continuously transformed to their nearest neighbor in the observable space.
%
\cite{Higgins2016} adopted this approach to their assumptions, that this notion can be generalized within the latent manifold learning.
%
Further, neither a coherent manifold nor a proper factorization of the latent space can be trained if these assumptions are not fulfilled by the dataset.
%
In summary, this means that observed data has to have the property of continues transformation wrt. to their properties (e.g. position and shape of an object), such that a small deviation of the observations results in proportional deviations in the latent space.
%
%, such that a coherent latent manifold can be trained.
% to learn a sufficient factorization in latent space.
% (i.e. is sampled from a )
% Moreover, we propose a technique to match-up existing datasets to meet the conditions of real raw multi-modal datasets.
%
We adopt this assumption for multi-modal datasets where observations should correlate if the same quantity is observed, such that a small deviation in the common latent representation between all modalities conducts a proportional impact in all observations.
%
This becomes an actual fundamental requirement for any multi-modal dataset, as correlation and coherence are within the objective of multi-modal sensor fusion.
%
%These assumptions only hold, iff the modality is able to rectify the target.
%
%This means speaking from the genrative model perspective for two modalities $a$ and $b$, there exists an likelihood $\p\(\a|z_1,z_2 \)$ and $\p\(\b|z_1\)$.
%
%However, in a factorized assumption the likelihoods become $\p\(\a|z_1,z_2\)$ and $\p\(\b|z_1,z_2\)$ and if $\b$ cannot sense a particular property, it is likely that it becomes independent of one factor (i.e. $\p\(\b|z_1\)$).
%
%This means that $\b$ makes ambiguous observations which can only be resolved if the rectification is done by $\a$ or $\a$ and $\b$ together.
%
%Moreover, we extend the postulation by Higgins et al. \cite{Higgins2016}, that it is important that the observed multi-modal data is generated using factors of variation that are densely sampled from their respective continuous distributions.
%
%In that case, and if the posterior is trained under the same prior, we propose that the overlay of latent factors suffice this assumption.
%
%In summary, that means that observed data has to have the property of continues transformation (i.e. is sampled from a ), such that to learn a sufficient factorization in latent space.
%
%Further more, a small change on the manifold of the posterior probability should result in a proportional deviation on the likelihood manifold.
%
%Or more over, a multi-modal observation should correlate if the same object is observed and moreover, the modalities can sense the object
%
%Here we specify a particular aspect of the data we believe is important for such learning.
%
%We postulate that it is important that the observed data is generated using factors of variation that are densely sampled from their respective continuous distributions.
%
In the following, we propose a technique to generate new multi-modal datasets, given different uni-modal enclosed sets which meet the former conditions.

A valuable property of the VAE's learned posterior distribution is, that it matches the desired prior quite sufficiently if only a single class is observed.
%
This characteristic can be found again in the conditional VAE (CVAE) \cite{Kingma2014,NIPS2015_5775} as it's training is supported by the ground truth labels of the observations.
%
Thus, it actually builds non-related posterior distribution for each class label, where every distribution matches a given prior.
%
Furthermore, we adopt the idea of $\beta$-VAE \cite{Higgins2017} which learns disentangled and factorized latent representations.
%
Combining the properties of both advantages allows the superimposing of latent manifolds from various uni-modal encoders as shown in Fig. \ref{fig:e_mnist} (Top-Right).
%
Now, latent samples can be drawn from the posterior to operate all CVAE encoders, with the desired label, to generate continues multi-modal data.
%

To test the approach we consolidate MNIST (\cite{LeCun1998}) and fashion-MNIST (\cite{Xiao2017}) to an entangled \textit{MNIST} (MNIST-E) set by sampling from the prior (i.e. $z\sim\mathcal{N}\( 0,\mathbf{I} \)$) to generate observation tuples from the corresponding encoder networks $\p_{\theta_{\a}}\(\a|z,C\)$ and $\p_{\theta_{\b}}\(\b|z,C\)$ with class label $C$.
%
The network architecture is explained in Sec. \ref{tab:architectures_emnist}.
%
To avoid artifacts, only samples from within $2\sigma$ of the prior are obtained.

Furthermore, we train a bi-modal JMVAE on the newly generated data to depict properties of the different datasets.
%
We are aware of the fact that consolidation of uni-modal datasets cannot be achieved easily since continuity is hardly measurable.
%
Therefore, na\"ive consolidation results in a mixed dataset (i.e. mixed-MNIST) as shown in Fig. \ref{fig:e_mnist}.
%
To mimic this behavior and to achieve a fair comparison of the ELBO, we shuffle the generated fashion-MNIST per class label of MNIST-E to generate an equivalent mixed \textit{MNIST-E} (MNIST-ME) set.
%

As shown in Fig. \ref{fig:e_mnist} (bottom), the JMVAE's latent space reveals that for MNIST-M single clusters share the same mean as the best representative of a single label, but the variance of any uni-modal trained encoder remains orthogonal.
%
Thus, the continuity in the observations does not correlate with each other by any means.
%
On the other hand, the MNIST-E set with continues samples shows the desired behavior of multi-modal datasets as the JMVAE trains a coherent distribution for all uni- and multi-modal encoders.
%
These observations show that our proposed approach for generating new entangled datasets meet the formulated requirements of multi-modal datasets.

%
%
%
%
%This can also be associated, to the necessary information channel which is used by the VAE.
%
%The VAE drives up the KL-divergence to 
%
%
%
%
\begin{figure}[h]
	\footnotesize
	\begin{center}
		%\input{VAEs_v2.pdf_tex}
		%\input{VAEs.pdf_tex}
		\input{MNIST_eval.pdf_tex}
	\end{center}
	\caption{\textbf{Top-Left}: Depiction of na\"ive mixed MNIST (m-MNIST) vs. proposed entangled MNIST (e-MNIST).
		%
		m-MNIST is pairwise plotted with the closest match of MNIST digits according to the mean-squared-error.
		%
		The corresponding fashion-MNIST samples show no continuity nor correlation (despite the intended class correlation).
		%
		e-MNIST shows the desired entanglement for changes of a single latent space factor.
		%
		\textbf{Top-Right}: Latent space of the CVAE for the modalities $\a$ (MNIST) and $\b$ (fashion-MNIST).
		%
		\textbf{Bottom}: Latent space of a trained JMVAE (c.f. Sec. \ref{sec:jmvae_e_mnist_setup}).
		%
		m-MNIST shows clear orthogonalization between modalities of the same class and segregation between classes (colorization is wrt. the CVAE legend).
		%
		e-MNIST shows a coherently learned latent space between the uni- and multi-modal encoders.
		%
		Thus, the JMVAE learns the correlation inside the dataset sufficiently ($\elbo_{\a,\b|\text{me-MNIST}} = -204.48$ vs. $\elbo_{\a,\b|\text{e-MNIST}} = -199.23$).
		}
	\label{fig:e_mnist}
\end{figure} 


%Data continuity We hypothesised that data continuity is important for guiding unsupervised models towards learning the correct data manifolds (Sec. 2). To test this idea we measured how the degree of learnt disentangling changes with reduced continuity in the 2D shapes dataset. We trained a VAE with Î² = 4 (Fig. 2A) on subsamples of the original 2D shapes dataset, where we progressively decreased the generative factor sampling density. Reduction in data continuity negatively correlates with the average pixel wise (Hamming) distance between two consecutive transforms of each object (normalised by the average number of pixels occupied by each of the two adjacent transforms of an object to account for object scale). Fig. 4A demonstrates that as the continuity in the data reduces, the degree of disentanglement in the learnt representations also drops. This effect holds after additional hyperparameter tuning and can not solely be explained by the decrease in dataset size, since the same VAE can learn disentangled representations from a data subset that preserves data continuity but is approximately 55 \% of the original size (see Sec. 3.4).
